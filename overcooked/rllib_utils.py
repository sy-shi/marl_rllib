import copy
import logging
import os
import random
import tempfile
from datetime import datetime
from PIL import Image
import os

import dill
import gym
import numpy as np
import ray
from ray.rllib.agents.ppo import PPOTrainer
from ray.rllib.algorithms.callbacks import DefaultCallbacks
from ray.rllib.env.multi_agent_env import MultiAgentEnv
from ray.rllib.models import ModelCatalog
from ray.tune.logger import UnifiedLogger
from ray.tune.registry import register_env
from ray.tune.result import DEFAULT_RESULTS_DIR
from overcooked_ai_py.planning.planners import NO_COUNTERS_PARAMS
from overcooked_ai_py.visualization.state_visualizer import StateVisualizer
from overcooked.utils import (
    get_base_ae,
    get_required_arguments,
    iterable_equal,
    softmax,
)
from overcooked_ai_py.agents.agent import Agent, AgentPair, AgentGroup
from overcooked_ai_py.agents.benchmarking import AgentEvaluator
from overcooked_ai_py.mdp.actions import Action
from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv
from overcooked_ai_py.mdp.overcooked_mdp import (
    EVENT_TYPES,
    OvercookedGridworld,
)
from overcooked.overcookedEnv import OvercookedMultiAgent

##################
# Training Utils #
##################

timestr = datetime.today().strftime("%Y-%m-%d_%H-%M-%S")

class TrainingCallbacks(DefaultCallbacks):
    def on_episode_start(self, worker, base_env, policies, episode, **kwargs):
        pass

    def on_episode_step(self, worker, base_env, episode, **kwargs):
        pass

    def on_episode_end(self, worker, base_env, policies, episode, **kwargs):
        """
        Used in order to add custom metrics to our tensorboard data

        sparse_reward (int) - total reward from deliveries agent earned this episode
        shaped_reward (int) - total reward shaping reward the agent earned this episode
        """
        # # Get rllib.OvercookedMultiAgentEnv refernce from rllib wraper
        # env = base_env.get_sub_environments()[0]
        # # Both agents share the same info so it doesn't matter whose we use, just use 0th agent's
        info_dict = episode.last_info_for(0)

        ep_info = info_dict["episode"]
        game_stats = ep_info["ep_game_stats"]

        # # List of episode stats we'd like to collect by agent
        # stats_to_collect = EVENT_TYPES

        # # Parse info dicts generated by OvercookedEnv
        tot_sparse_reward = ep_info["ep_sparse_r"]
        tot_shaped_reward = ep_info["ep_shaped_r"]

        # # Store metrics where they will be visible to rllib for tensorboard logging
        episode.custom_metrics["sparse_reward"] = tot_sparse_reward
        episode.custom_metrics["shaped_reward"] = tot_shaped_reward


class AgentEvaluatorMultiAgent(AgentEvaluator):
    """
    Class used to get rollouts and evaluate performance of various types of agents.

    TODO: This class currently only fully supports fixed mdps, or variable mdps that can be created with the LayoutGenerator class,
    but might break with other types of variable mdps. Some methods currently assume that the AgentEvaluator can be reconstructed
    from loaded params (which must be pickleable). However, some custom start_state_fns or mdp_generating_fns will not be easily
    pickleable. We should think about possible improvements/what makes most sense to do here.
    """

    def __init__(self, config, force_compute=False):
        """
        env_params (dict): params for creation of an OvercookedEnv
        mdp_fn (callable function): a function that can be used to create mdp
        force_compute (bool): whether should re-compute MediumLevelActionManager although matching file is found
        mlam_params (dict): the parameters for mlam, the MediumLevelActionManager
        debug (bool): whether to display debugging information on init
        """
        self.env = OvercookedMultiAgent(config)
        self.visualizer = StateVisualizer(player_colors = ['red', 'green', "blue"])


class RlLibAgent(Agent):
    """
    Class for wrapping a trained RLLib Policy object into an Overcooked compatible Agent
    """

    def __init__(self, policy, agent_index, featurize_fn):
        self.policy = policy
        self.agent_index = agent_index
        self.featurize = featurize_fn

    def reset(self):
        # Get initial rnn states and add batch dimension to each
        if hasattr(self.policy.model, "get_initial_state"):
            self.rnn_state = [
                np.expand_dims(state, axis=0)
                for state in self.policy.model.get_initial_state()
            ]
        elif hasattr(self.policy, "get_initial_state"):
            self.rnn_state = [
                np.expand_dims(state, axis=0)
                for state in self.policy.get_initial_state()
            ]
        else:
            self.rnn_state = []

    def action_probabilities(self, state):
        """
        Arguments:
            - state (Overcooked_mdp.OvercookedState) object encoding the global view of the environment
        returns:
            - Normalized action probabilities determined by self.policy
        """
        # Preprocess the environment state
        obs = self.featurize(state, debug=False)
        my_obs = obs[self.agent_index]
        
        # Compute non-normalized log probabilities from the underlying model
        logits = self.policy.compute_actions(
            np.array([my_obs]), self.rnn_state
        )[2]["action_dist_inputs"]

        # Softmax in numpy to convert logits to normalized probabilities
        return softmax(logits)

    def action(self, state):
        """
        Arguments:
            - state (Overcooked_mdp.OvercookedState) object encoding the global view of the environment
        returns:
            - the argmax action for a single observation state
            - action_info (dict) that stores action probabilities under 'action_probs' key
        """
        # Preprocess the environment state
        obs = self.featurize(state)
        my_obs = obs[self.agent_index]

        # Use Rllib.Policy class to compute action argmax and action probabilities
        # The first value is action_idx, which we will recompute below so the results are stochastic
        # print(my_obs)
        _, rnn_state, info = self.policy.compute_actions(
            my_obs, self.rnn_state
        )

        # Softmax in numpy to convert logits to normalized probabilities
        logits = info["action_dist_inputs"]
        action_probabilities = softmax(logits)

        # The original design is stochastic across different games,
        # Though if we are reloading from a checkpoint it would inherit the seed at that point, producing deterministic results
        [action_idx] = random.choices(
            list(range(Action.NUM_ACTIONS)), action_probabilities[0]
        )
        agent_action = Action.INDEX_TO_ACTION[action_idx]

        agent_action_info = {"action_probs": action_probabilities}
        self.rnn_state = rnn_state

        return agent_action, agent_action_info

def evaluate(
    config,
    policies,
    num_episodes,
    display,
    ifsave=False,
    save=None,
    verbose=True,
):
    if verbose:
        print("eval mdp params", config)
    evaluator = AgentEvaluatorMultiAgent(
        config=config
    )
    agents = []
    for agent in evaluator.env.agents:
        agents.append(RlLibAgent(policies[agent], agent, evaluator.env.get_obs))
    
    results = evaluator.evaluate_agent_pair(
        AgentGroup(*agents),
        num_games=num_episodes,
        display=display,
        dir= (None if not ifsave else save),
        display_phi=False,
        info=verbose,
    )
    if display:
        for idx in range(num_episodes):
            evaluator.visualizer.display_rendered_trajectory(trajectories=results,trajectory_idx=idx, img_directory_path='./data/overcooked_vis/')
            img_to_gif('./data/overcooked_vis_cramped_room/', f"episode_{idx}", fps=4)
    return results



def img_to_gif(image_folder, output_name, fps):
    # Get the list of image filenames and sort them
    image_files = [f"{i}.png" for i in range(600)]
    image_files = [os.path.join(image_folder, file) for file in image_files if os.path.isfile(os.path.join(image_folder, file))]

    # Open the images and store them in a list
    images = [Image.open(img) for img in image_files]

    # Set the duration for each frame (1000 ms / 5 frames = 200 ms per frame)
    duration_per_frame = 1000/fps  # 200 ms = 5 frames per second

    # Save as GIF
    gif_output_path = os.path.join(image_folder, output_name+'.gif')
    images[0].save(
        gif_output_path,
        save_all=True,
        append_images=images[1:],  # Append the rest of the images
        duration=duration_per_frame,
        loop=0  # Infinite loop
    )